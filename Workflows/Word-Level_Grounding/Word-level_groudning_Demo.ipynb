{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level Grounding Using Agentic Document Extraction (ADE) from LandingAI\n",
    "\n",
    "Extract a single field from a PDF and visualize where the source on a word-level for a more precise grounding, using ADE's Parsing & Field Extraction Python Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install landingai-ade python-dotenv pillow pymupdf matplotlib pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pymupdf\n",
    "import matplotlib.pyplot as plt\n",
    "import pytesseract\n",
    "\n",
    "from landingai_ade import LandingAIADE\n",
    "from landingai_ade.lib import pydantic_to_json_schema\n",
    "\n",
    "load_dotenv()\n",
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Document (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LandingAIADE(apikey=os.environ.get(\"VISION_AGENT_API_KEY\"))\n",
    "pdf_path = Path(\"demo_pdf.pdf\")\n",
    "\n",
    "print(f\"Parsing {pdf_path}...\")\n",
    "parse_response = client.parse(\n",
    "    document=pdf_path,\n",
    "    model=os.environ.get(\"ADE_MODEL\", \"dpt-2-latest\"),\n",
    ")\n",
    "\n",
    "print(f\"Parsed successfully!\")\n",
    "print(f\"  - Markdown: {len(parse_response.markdown)} chars\")\n",
    "print(f\"  - Chunks: {len(parse_response.chunks)}\")\n",
    "\n",
    "# Save markdown to file\n",
    "markdown_path = Path(\"parsed_output.md\")\n",
    "with open(markdown_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(parse_response.markdown)\n",
    "print(f\"Saved markdown to {markdown_path}\")\n",
    "\n",
    "# Save grounding data for visualization\n",
    "grounding_data = {}\n",
    "for chunk_id, grounding in parse_response.grounding.items():\n",
    "    grounding_data[chunk_id] = {\n",
    "        'type': grounding.type,\n",
    "        'page': grounding.page,\n",
    "        'box': {\n",
    "            'left': grounding.box.left,\n",
    "            'top': grounding.box.top,\n",
    "            'right': grounding.box.right,\n",
    "            'bottom': grounding.box.bottom\n",
    "        }\n",
    "    }\n",
    "\n",
    "grounding_path = Path(\"grounding_data.json\")\n",
    "with open(grounding_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grounding_data, f, indent=2)\n",
    "print(f\"Saved grounding data to {grounding_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting field from saved markdown file...\n",
      "\n",
      "============================================================\n",
      "EXTRACTED DATA\n",
      "============================================================\n",
      "\n",
      "Remaining Human Genome: 8%\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "client = LandingAIADE(apikey=os.environ.get(\"VISION_AGENT_API_KEY\"))\n",
    "\n",
    "print(\"Extracting field from saved markdown file...\")\n",
    "\n",
    "# Prepare schema as dict (not json string)\n",
    "schema_dict = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"remaining_human_genome\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The percentage of the human genome that was left unfinished before the Telomere-to-Telomere (T2T) Consortium's work\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"remaining_human_genome\"]\n",
    "}\n",
    "\n",
    "# Extract from the saved markdown file\n",
    "markdown_path = Path(\"parsed_output.md\")\n",
    "extract_response = client.extract(\n",
    "    schema=json.dumps(schema_dict),\n",
    "    markdown=markdown_path\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXTRACTED DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRemaining Human Genome: {extract_response.extraction.get('remaining_human_genome', 'N/A')}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 page(s)\n"
     ]
    }
   ],
   "source": [
    "# Convert PDF to images\n",
    "pdf_path = Path(\"demo_pdf.pdf\")\n",
    "pdf = pymupdf.open(pdf_path)\n",
    "page_images = {}\n",
    "\n",
    "for page_num in range(len(pdf)):\n",
    "    page = pdf[page_num]\n",
    "    pix = page.get_pixmap(matrix=pymupdf.Matrix(2, 2))\n",
    "    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "    page_images[page_num] = img\n",
    "\n",
    "pdf.close()\n",
    "print(f\"Loaded {len(page_images)} page(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 grounding entries\n"
     ]
    }
   ],
   "source": [
    "# Load grounding data for visualization\n",
    "with open(\"grounding_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    grounding_data = json.load(f)\n",
    "\n",
    "# Reconstruct grounding objects\n",
    "class Grounding:\n",
    "    def __init__(self, data):\n",
    "        self.type = data['type']\n",
    "        self.page = data['page']\n",
    "        self.box = type('Box', (), data['box'])()\n",
    "\n",
    "grounding_dict = {chunk_id: Grounding(data) for chunk_id, data in grounding_data.items()}\n",
    "print(f\"Loaded {len(grounding_dict)} grounding entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Chunks with Extracted Value\n",
    "\n",
    "Check extraction_metadata for chunk references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pytesseract\n",
    "\n",
    "# Get the extracted value\n",
    "extracted_value = extract_response.extraction.get('remaining_human_genome', '')\n",
    "print(f\"Extracted value: '{extracted_value}'\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Check extraction_metadata for chunk references\n",
    "print(\"\\nChecking extraction_metadata...\")\n",
    "chunk_ids = []\n",
    "\n",
    "if hasattr(extract_response, 'extraction_metadata') and extract_response.extraction_metadata:\n",
    "    metadata = extract_response.extraction_metadata\n",
    "    \n",
    "    # Check if our field has references\n",
    "    if isinstance(metadata, dict) and 'remaining_human_genome' in metadata:\n",
    "        field_meta = metadata['remaining_human_genome']\n",
    "        print(f\"Field metadata: {field_meta}\")\n",
    "        \n",
    "        # Look for 'references' key (not 'chunk_references')\n",
    "        if isinstance(field_meta, dict) and 'references' in field_meta:\n",
    "            chunk_ids = field_meta['references']\n",
    "            print(f\"\\nâœ“ Found {len(chunk_ids)} chunk reference(s) from API: {chunk_ids}\")\n",
    "        else:\n",
    "            print(f\"No 'references' key in field metadata\")\n",
    "    else:\n",
    "        print(f\"Field not in metadata. Keys: {metadata.keys() if isinstance(metadata, dict) else 'N/A'}\")\n",
    "\n",
    "# Fallback: search all chunks\n",
    "if not chunk_ids:\n",
    "    print(\"\\nNo chunk references from API. Will search all chunks.\")\n",
    "    chunk_ids = list(grounding_dict.keys())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Will OCR {len(chunk_ids)} chunk(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_ratio(text1, text2):\n",
    "    \"\"\"Return similarity ratio between two strings (0-1)\"\"\"\n",
    "    return SequenceMatcher(None, text1.lower().strip(), text2.lower().strip()).ratio()\n",
    "\n",
    "print(\"Running OCR on identified chunks...\")\n",
    "ocr_matches = []\n",
    "\n",
    "# Process each identified chunk\n",
    "for chunk_id in chunk_ids:\n",
    "    if chunk_id not in grounding_dict:\n",
    "        continue\n",
    "    \n",
    "    grounding = grounding_dict[chunk_id]\n",
    "    page_num = grounding.page\n",
    "    \n",
    "    if page_num not in page_images:\n",
    "        continue\n",
    "    \n",
    "    img = page_images[page_num]\n",
    "    img_width, img_height = img.size\n",
    "    \n",
    "    # Get chunk bounding box in pixels\n",
    "    box = grounding.box\n",
    "    x1 = int(box.left * img_width)\n",
    "    y1 = int(box.top * img_height)\n",
    "    x2 = int(box.right * img_width)\n",
    "    y2 = int(box.bottom * img_height)\n",
    "    \n",
    "    # Crop image to chunk area\n",
    "    chunk_img = img.crop((x1, y1, x2, y2))\n",
    "    \n",
    "    # Run OCR only on this chunk\n",
    "    ocr_data = pytesseract.image_to_data(chunk_img, output_type=pytesseract.Output.DICT)\n",
    "    \n",
    "    # Process OCR results\n",
    "    for i in range(len(ocr_data['text'])):\n",
    "        text = ocr_data['text'][i].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        conf = float(ocr_data['conf'][i])\n",
    "        if conf < 30:\n",
    "            continue\n",
    "        \n",
    "        # Fuzzy match with extracted value\n",
    "        similarity = fuzzy_match_ratio(extracted_value, text)\n",
    "        \n",
    "        # Boost for substring matches\n",
    "        if extracted_value.lower() in text.lower() or text.lower() in extracted_value.lower():\n",
    "            similarity = max(similarity, 0.85)\n",
    "        \n",
    "        if similarity > 0.5:\n",
    "            # Convert coordinates back to full image space\n",
    "            ocr_x = x1 + ocr_data['left'][i]\n",
    "            ocr_y = y1 + ocr_data['top'][i]\n",
    "            ocr_w = ocr_data['width'][i]\n",
    "            ocr_h = ocr_data['height'][i]\n",
    "            \n",
    "            ocr_matches.append({\n",
    "                'text': text,\n",
    "                'similarity': similarity,\n",
    "                'confidence': conf,\n",
    "                'page': page_num,\n",
    "                'chunk_id': chunk_id,\n",
    "                'box_pixels': {'x': ocr_x, 'y': ocr_y, 'w': ocr_w, 'h': ocr_h}\n",
    "            })\n",
    "\n",
    "# Sort by similarity\n",
    "ocr_matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "print(f\"\\nFound {len(ocr_matches)} OCR matches in {len(chunk_ids)} chunk(s):\")\n",
    "for match in ocr_matches[:10]:\n",
    "    print(f\"  - '{match['text']}': {match['similarity']:.1%} similarity, {match['confidence']:.0f}% confidence\")\n",
    "    print(f\"    Chunk {match['chunk_id']}, Page {match['page']}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create heatmap visualization using OCR matches\n",
    "page_num = 0\n",
    "img = page_images[page_num].copy().convert('RGBA')\n",
    "img_width, img_height = img.size\n",
    "\n",
    "# Create a transparent overlay for highlighter effect\n",
    "overlay = Image.new('RGBA', img.size, (0, 0, 0, 0))\n",
    "overlay_draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "# Draw all chunks in very light gray (subtle context)\n",
    "for chunk_id, grounding_obj in grounding_dict.items():\n",
    "    if grounding_obj.page != page_num:\n",
    "        continue\n",
    "    box = grounding_obj.box\n",
    "    x1 = int(box.left * img_width)\n",
    "    y1 = int(box.top * img_height)\n",
    "    x2 = int(box.right * img_width)\n",
    "    y2 = int(box.bottom * img_height)\n",
    "    overlay_draw.rectangle([x1, y1, x2, y2], outline=(220, 220, 220, 60), width=1)\n",
    "\n",
    "# Highlighter effect for OCR matches\n",
    "for match in ocr_matches:\n",
    "    if match['page'] != page_num:\n",
    "        continue\n",
    "    \n",
    "    box_px = match['box_pixels']\n",
    "    x1, y1 = box_px['x'], box_px['y']\n",
    "    x2, y2 = x1 + box_px['w'], y1 + box_px['h']\n",
    "    \n",
    "    # Add padding for highlighter effect (like a marker pen)\n",
    "    padding = 2\n",
    "    x1 -= padding\n",
    "    y1 -= padding\n",
    "    x2 += padding\n",
    "    y2 += padding\n",
    "    \n",
    "    similarity = match['similarity']\n",
    "    \n",
    "    # Transparent yellow highlighter (like a marker)\n",
    "    # Higher similarity = more opaque\n",
    "    if similarity > 0.9:\n",
    "        # Bright yellow for exact matches\n",
    "        color = (255, 255, 0, 120)  # Yellow highlighter\n",
    "    elif similarity > 0.7:\n",
    "        # Orange highlighter for good matches\n",
    "        color = (255, 200, 0, 100)\n",
    "    else:\n",
    "        # Light yellow for partial matches\n",
    "        color = (255, 255, 100, 80)\n",
    "    \n",
    "    # Draw filled rectangle (highlighter effect - no outline)\n",
    "    overlay_draw.rectangle([x1, y1, x2, y2], fill=color)\n",
    "    \n",
    "    # Optional: add small similarity badge\n",
    "    if similarity > 0.8:\n",
    "        label = f\"{similarity:.0%}\"\n",
    "        label_x = x2 + 2\n",
    "        label_y = y1\n",
    "        # Small red badge\n",
    "        overlay_draw.ellipse([label_x, label_y, label_x+20, label_y+20], fill=(255, 0, 0, 200))\n",
    "        overlay_draw.text((label_x+3, label_y+3), label, fill=(255, 255, 255, 255))\n",
    "\n",
    "# Composite overlay with base image\n",
    "img = Image.alpha_composite(img, overlay)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(14, 18))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Highlighted: '{extracted_value}' ({len(ocr_matches)} matches found)\", \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nHighlighted {len(ocr_matches)} word-level matches\")\n",
    "if ocr_matches:\n",
    "    best = ocr_matches[0]\n",
    "    print(f\"Best match: '{best['text']}' ({best['similarity']:.0%} similarity, {best['confidence']:.0f}% confidence)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
