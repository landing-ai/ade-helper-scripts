{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDl9wiBcCMlq"
      },
      "source": [
        "# Introducing Parse Jobs API - Quick Start Guide\n",
        "\n",
        "**Process *large* documents asynchronously with LandingAI ADE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU88Fv0pCMlt"
      },
      "outputs": [],
      "source": [
        "# ---\n",
        "# LandingAI Applied AI Content Notebook Template\n",
        "# ---\n",
        "# Title: Introducing Parse Jobs API - Quick Start Guide\n",
        "# Author: Ava Xia\n",
        "# Description: Simple introduction to ADE Parse Jobs API for processing large documents\n",
        "# Target Audience: Developers\n",
        "# Content Type: Tutorial\n",
        "# Publish Date: 2025-10-10\n",
        "# ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgsR0llHCMlu"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The **ADE Parse Jobs API** enables processing of large documents (up to 1GB / 1,000 pages) that exceed the limits of the standard synchronous API.\n",
        "\n",
        "### Key Benefits:\n",
        "- 📄 **Large Documents**: Up to 1GB files and 1,000 pages\n",
        "- ⚡ **Non-blocking**: Submit and check status later\n",
        "- 📊 **Progress Tracking**: Monitor completion (0.0 to 1.0)\n",
        "- 🔄 **Automatic Retries**: Built-in error handling\n",
        "\n",
        "### Parse vs Parse Jobs Comparison:\n",
        "\n",
        "| Feature | Standard Parse API | Parse Jobs API |\n",
        "|---------|----------|----------|\n",
        "| Max Size | 50MB | 1GB |\n",
        "| Max Pages | 50 | 1,000 |\n",
        "| Response | Immediate | Job ID |\n",
        "| Best For | Small docs | Large docs |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tn23zF8CMlv"
      },
      "source": [
        "## 🔧 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HERYxlCxCMlv",
        "outputId": "b3af7b09-7bb4-491b-cd74-1bc095a52582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# Install/upgrade required libraries: `requests` (HTTP client) and `python-dotenv` (loads .env files)\n",
        "!pip install -U requests python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMQF9ibbCMlv",
        "outputId": "5510e0d3-75e8-45df-f0ba-0f121a3bd87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API Key configured\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration of the LandingAI API key\n",
        "API_KEY = os.getenv('VISION_AGENT_API_KEY')\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"⚠️ Please set VISION_AGENT_API_KEY environment variable\")\n",
        "    print(f\"Get your key at: https://docs.landing.ai/ade/agentic-api-key\")\n",
        "else:\n",
        "    print(\"✅ API Key configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = 'https://api.va.landing.ai/'"
      ],
      "metadata": {
        "id": "Ybfk2O2MVKFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvUea-fdCMlw"
      },
      "source": [
        "## 🔄  Workflow\n",
        "\n",
        "The parse jobs API follows a simple 3-step workflow:\n",
        "\n",
        "```\n",
        "1. SUBMIT → Returns job_id\n",
        "2. MONITOR → Check progress\n",
        "3. RETRIEVE → Get results\n",
        "4. Integrated Workflow → (Optional) Assemble these three steps into one\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF1mDVSjCMlw"
      },
      "source": [
        "## 📤 Step 1: Submit Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOR7lv9JCMlw"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from pathlib import Path\n",
        "import requests, os\n",
        "\n",
        "def submit_document(file_path: str, api_key: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Upload a PDF to LandingAI’s ADE endpoint and get back the job_id.\n",
        "    Returns the job_id string when the POST succeeds, else None.\n",
        "    \"\"\"\n",
        "    # ── 1.  Resolve & sanity-check the path ───────────────────────────────────\n",
        "    p = Path(file_path).expanduser().resolve()\n",
        "    if not p.exists():\n",
        "        print(f\"❌ File not found: {p}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"📄 File: {p.name}  |  📏 {p.stat().st_size / 1_048_576:.1f} MB\")\n",
        "\n",
        "    # ── 2.  Prepare request ──────────────────────────────────────────────────\n",
        "    url = f'{BASE_URL}/v1/ade/parse/jobs'\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "    # open() inside a with-block → auto-close even if an exception fires\n",
        "    with p.open(\"rb\") as fh:\n",
        "        files = {\"document\": fh}\n",
        "        resp  = requests.post(url, headers=headers, files=files, timeout=30)\n",
        "\n",
        "    # ── 3.  Handle response ──────────────────────────────────────────────────\n",
        "    if resp.status_code in (200, 202):                    # 202 = queued/accepted\n",
        "        data   = resp.json()\n",
        "        job_id = data.get(\"job_id\")\n",
        "        if job_id:\n",
        "            print(f\"✅ Job accepted — job_id: {job_id}\")\n",
        "            return job_id\n",
        "        else:\n",
        "            print(\"❌ Response missing job_id:\", data)\n",
        "            return None\n",
        "\n",
        "    # Any non-success status drops through to here\n",
        "    print(f\"❌ Upload failed ({resp.status_code}): {resp.text}\")\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FmKMg0GCMlx",
        "outputId": "9a762f74-eb4f-4505-c7e1-2c2f28d9e2ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 File: one_huge_file.pdf  |  📏 57.6 MB\n",
            "✅ Job accepted — job_id: cmgmldz540004mj8do5uwzf0l\n"
          ]
        }
      ],
      "source": [
        "job_id = submit_document('one_huge_file.pdf', API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHX52DKiCMlx"
      },
      "source": [
        "## 📊 Step 2: Monitor Job Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGBfjzHKCMlx"
      },
      "outputs": [],
      "source": [
        "def check_job_status(job_id: str, api_key: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Check the status of an async job.\n",
        "\n",
        "    Args:\n",
        "        job_id: The job ID from submission\n",
        "        api_key: Your API key\n",
        "\n",
        "    Returns:\n",
        "        Status dictionary with progress and results\n",
        "    \"\"\"\n",
        "    url = f'{BASE_URL}/v1/ade/parse/jobs/{job_id}'\n",
        "    headers = {'Authorization': f'Bearer {api_key}'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            status = data.get('status')\n",
        "            progress = data.get('progress', 0) * 100\n",
        "\n",
        "            print(f\"Status: {status} | Progress: {progress:.0f}%\")\n",
        "            return data\n",
        "        else:\n",
        "            print(f\"❌ Error checking status: {response.status_code}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Poll until complete\n",
        "def wait_for_completion(job_id: str, api_key: str, timeout: int = 3600):\n",
        "    \"\"\"\n",
        "    Wait for job to complete with polling.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    while time.time() - start_time < timeout:\n",
        "        status_data = check_job_status(job_id, api_key)\n",
        "\n",
        "        if status_data:\n",
        "            status = status_data.get('status')\n",
        "\n",
        "            if status == 'completed':\n",
        "                print(\"✅ Job completed!\")\n",
        "                return status_data\n",
        "            elif status == 'failed':\n",
        "                print(f\"❌ Job failed: {status_data.get('failure_reason')}\")\n",
        "                return None\n",
        "\n",
        "        time.sleep(30)  # Poll every 30 seconds\n",
        "\n",
        "    print(\"⏱️ Timeout waiting for completion\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "ChIzjUPIOP-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "result = wait_for_completion(job_id, API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtqLtnk0OUEG",
        "outputId": "aff4cd9e-0d15-4f40-895e-8c9081c52443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: processing | Progress: 0%\n",
            "Status: processing | Progress: 5%\n",
            "Status: processing | Progress: 7%\n",
            "Status: processing | Progress: 12%\n",
            "Status: processing | Progress: 17%\n",
            "Status: processing | Progress: 19%\n",
            "Status: processing | Progress: 21%\n",
            "Status: processing | Progress: 26%\n",
            "Status: processing | Progress: 28%\n",
            "Status: processing | Progress: 31%\n",
            "Status: processing | Progress: 36%\n",
            "Status: processing | Progress: 38%\n",
            "Status: processing | Progress: 40%\n",
            "Status: processing | Progress: 45%\n",
            "Status: processing | Progress: 48%\n",
            "Status: processing | Progress: 52%\n",
            "Status: processing | Progress: 55%\n",
            "Status: processing | Progress: 59%\n",
            "Status: processing | Progress: 62%\n",
            "Status: processing | Progress: 64%\n",
            "Status: processing | Progress: 67%\n",
            "Status: processing | Progress: 69%\n",
            "Status: processing | Progress: 74%\n",
            "Status: processing | Progress: 76%\n",
            "Status: processing | Progress: 78%\n",
            "Status: processing | Progress: 83%\n",
            "Status: processing | Progress: 86%\n",
            "Status: processing | Progress: 88%\n",
            "Status: processing | Progress: 90%\n",
            "Status: processing | Progress: 95%\n",
            "Status: processing | Progress: 100%\n",
            "Status: completed | Progress: 100%\n",
            "✅ Job completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlYDEwKpCMly"
      },
      "source": [
        "## 📥 Step 3: Retrieve Results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling API Results\n",
        "\n",
        "The API provides job results in two different ways to optimize performance. For smaller documents, the results are returned directly in the status response. For larger documents, a temporary download link is provided via the `output_url` field.\n",
        "\n",
        "- **`output_url`**: `string | null`  \n",
        "  The `output_url` is a field in the API response that contains a secure, temporary link to download your job's results. Its value will be either a **string** (the URL) or **`null`**.\n",
        "\n",
        "Think of it like receiving mail. 📬 A small letter fits directly into your mailbox, but for a large package, you get a slip telling you where to pick it up.\n",
        "\n",
        "- **Direct Results (Small Files < 1 MB)**:  \n",
        "  If the processed output is small, the API returns it directly within the `data` field of the response. In this case, `output_url` will be `null`.\n",
        "\n",
        "- **URL Link (Large Files ≥ 1 MB)**:  \n",
        "  If the output is large, the API places the results in a separate JSON file and provides a link to it in the `output_url` field. This prevents the main API response from becoming slow or unwieldy. In this scenario, the `data` field will be `null`.\n",
        "\n",
        "> **Note:**  \n",
        "> The URL is generated only when the job’s `status` is **`completed`**. To get the results, make a separate HTTP **GET** request to this URL to download the JSON file containing the final markdown content."
      ],
      "metadata": {
        "id": "sTaj3_1lrSTM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjJ4_8a9CMly"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from typing import Optional\n",
        "\n",
        "def get_results(job_id: str, api_key: str, save_to_file: bool = True) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Retrieve results from a completed job, handling both direct data responses\n",
        "    (for small files) and fetching from an output URL (for large files).\n",
        "\n",
        "    Args:\n",
        "        job_id: The job ID.\n",
        "        api_key: Your API key.\n",
        "        save_to_file: Whether to save the markdown content to a file.\n",
        "\n",
        "    Returns:\n",
        "        Markdown content if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    # 1. Check the job status\n",
        "    status_data = check_job_status(job_id, api_key)\n",
        "    if not status_data or status_data.get('status') != 'completed':\n",
        "        status = status_data.get('status', 'unknown') if status_data else 'unknown'\n",
        "        print(f\"⚠️ Job not completed yet. Current status: '{status}'\")\n",
        "        return None\n",
        "\n",
        "    markdown = ''\n",
        "\n",
        "    # 2. Check if results are returned directly (for smaller files)\n",
        "    if status_data.get('data') is not None:\n",
        "        print(\"✅ Job complete. Results found directly in API response.\")\n",
        "        data = status_data.get('data', {})\n",
        "        markdown = data.get('markdown', '')\n",
        "\n",
        "    # 3. If not, fetch results from the output URL (for larger files)\n",
        "    else:\n",
        "        output_url = status_data.get('output_url')\n",
        "        if not output_url:\n",
        "            print(\"❌ Job is complete, but no output URL or direct data was found.\")\n",
        "            return None\n",
        "\n",
        "        print(\"✅ Job complete. Fetching results from URL for large file...\")\n",
        "        try:\n",
        "            response = requests.get(output_url)\n",
        "            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "            results_data = response.json()\n",
        "            markdown = results_data.get('markdown', '')\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Failed to fetch results from URL: {e}\")\n",
        "            return None\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"❌ Failed to parse the fetched results as JSON.\")\n",
        "            return None\n",
        "\n",
        "    # 4. Process the markdown, regardless of where it came from\n",
        "    if markdown:\n",
        "        print(f\"📄 Retrieved {len(markdown)} characters of markdown.\")\n",
        "\n",
        "        if save_to_file:\n",
        "            output_file = f'{job_id}_output.md'\n",
        "            with open(output_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(markdown)\n",
        "            print(f\"💾 Saved to: {output_file}\")\n",
        "\n",
        "        # Display metadata from the top-level status object\n",
        "        metadata = status_data.get('metadata', {})\n",
        "        if metadata:\n",
        "            print(f\"\\n📊 Processing stats:\")\n",
        "            print(f\"  • Pages: {metadata.get('page_count', 'N/A')}\")\n",
        "            print(f\"  • Time: {metadata.get('duration_ms', 0) / 1000:.1f}s\")\n",
        "            print(f\"  • Credits: {metadata.get('credit_usage', 'N/A')}\")\n",
        "\n",
        "        return markdown\n",
        "    else:\n",
        "        print(\"❌ No markdown content found in the results.\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example Usage ---\n",
        "markdown_content = get_results(job_id, API_KEY, save_to_file=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee85HH9ujB1-",
        "outputId": "30d388c2-24bf-41a9-b1c4-d0ca5ea92feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Status: completed | Progress: 100%\n",
            "✅ Job complete. Fetching results from URL for large file...\n",
            "📄 Retrieved 1331672 characters of markdown.\n",
            "💾 Saved to: cmgmldz540004mj8do5uwzf0l_output.md\n",
            "\n",
            "📊 Processing stats:\n",
            "  • Pages: 421\n",
            "  • Time: 946.0s\n",
            "  • Credits: 1263.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Function to Preview the Result"
      ],
      "metadata": {
        "id": "XIbvU8ZTuf19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preview_markdown(file_path: str, num_chars: int = 1000):\n",
        "    \"\"\"\n",
        "    Prints the first few characters of a text file.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the file you want to preview.\n",
        "        num_chars: The number of characters to display.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            # Read the specified number of characters\n",
        "            preview_content = f.read(num_chars)\n",
        "\n",
        "            print(f\"📄 Previewing first {num_chars} characters of '{file_path}':\")\n",
        "            print(\"--------------------- START OF FILE ---------------------\")\n",
        "            print(preview_content)\n",
        "            print(\"---------------------- END OF PREVIEW ----------------------\")\n",
        "\n",
        "            # Add a small note if the preview is the same length as requested,\n",
        "            # which implies there's probably more content in the file.\n",
        "            if len(preview_content) == num_chars:\n",
        "                print(\"(File continues...)\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"❌ Error: The file '{file_path}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "sbUGdfrwkwSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the job_id from the previous step to construct the filename\n",
        "job_id = 'cmgmldz540004mj8do5uwzf0l'\n",
        "markdown_filename = f'{job_id}_output.md'\n",
        "\n",
        "# Call the function to print the first 1000 characters\n",
        "preview_markdown(markdown_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8vuJU1qk1xj",
        "outputId": "3f48f550-9858-40e3-f2d2-1ce818f6f988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Previewing first 1000 characters of 'cmgmldz540004mj8do5uwzf0l_output.md':\n",
            "--------------------- START OF FILE ---------------------\n",
            "<a id='8f83a392-9759-49c1-b816-aade0dbe1a8f'></a>\n",
            "\n",
            "MACHINE\n",
            "LEARNING\n",
            "\n",
            "<::A diagram resembling a neural network with interconnected nodes.\n",
            ": figure::>\n",
            "\n",
            "TOM M. MITCHELL\n",
            "\n",
            "<a id='99a96c5b-2a2e-44ad-bf1b-da9878254eb6'></a>\n",
            "\n",
            "Machine Learning\n",
            "\n",
            "<a id='bca724ab-3e4b-414d-9cc8-d923d7882a40'></a>\n",
            "\n",
            "Tom M. Mitchell\n",
            "\n",
            "<a id='a85d4599-c8e4-46d6-8c12-bcfaba697827'></a>\n",
            "\n",
            "## Product Details\n",
            "*   **Hardcover**: 432 pages ; Dimensions (in inches): 0.75 x 10.00 x 6.50\n",
            "*   **Publisher**: McGraw-Hill Science/Engineering/Math; (March 1, 1997)\n",
            "*   **ISBN**: 0070428077\n",
            "*   **Average Customer Review**: ⭐⭐⭐⭐⭒ Based on 16 reviews.\n",
            "*   **Amazon.com Sales Rank**: 42,816\n",
            "*   **Popular in**: Redmond, WA (#17), Ithaca, NY (#9)\n",
            "\n",
            "<a id='8d37667c-0bd9-4920-a7ac-b4c1cf2a7db9'></a>\n",
            "\n",
            "## Editorial Reviews\n",
            "\n",
            "**From Book News, Inc.** An introductory text on primary approaches to machine learning and the study of computer algorithms that improve automatically through experience. Introduce basics concepts from statistics, artificial \n",
            "---------------------- END OF PREVIEW ----------------------\n",
            "(File continues...)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X6H_DqNCMly"
      },
      "source": [
        "## Step 4: Assemble the Entire Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiKH6lLECMly"
      },
      "outputs": [],
      "source": [
        "def process_large_document(file_path: str, api_key: str):\n",
        "    \"\"\"\n",
        "    Complete workflow for processing a large document.\n",
        "    \"\"\"\n",
        "    print(\"🚀 ASYNC DOCUMENT PROCESSING WORKFLOW\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Step 1: Submit\n",
        "    print(\"\\n1️⃣ Submitting document...\")\n",
        "    job_id = submit_document(file_path, api_key)\n",
        "\n",
        "    if not job_id:\n",
        "        print(\"Failed to submit document\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Wait for completion\n",
        "    print(\"\\n2️⃣ Waiting for processing...\")\n",
        "    result = wait_for_completion(job_id, api_key)\n",
        "\n",
        "    if not result:\n",
        "        print(\"Processing failed or timed out\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Get results\n",
        "    print(\"\\n3️⃣ Retrieving results...\")\n",
        "    markdown = get_results(job_id, api_key)\n",
        "\n",
        "    if not markdown:\n",
        "        print(\"Failed to retrieve results\")\n",
        "        return\n",
        "\n",
        "    return {\n",
        "        'job_id': job_id,\n",
        "        'markdown': markdown,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "results = process_large_document('one_large_file.pdf', API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jMtrSl8PxK4",
        "outputId": "df06587f-81a9-4772-a520-460edf69fc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 ASYNC DOCUMENT PROCESSING WORKFLOW\n",
            "==================================================\n",
            "\n",
            "1️⃣ Submitting document...\n",
            "📄 File: one_large_file.pdf  |  📏 39.0 MB\n",
            "✅ Job accepted — job_id: cmgmnkyer0004f4ee511uo9oc\n",
            "\n",
            "2️⃣ Waiting for processing...\n",
            "Status: processing | Progress: 0%\n",
            "Status: processing | Progress: 10%\n",
            "Status: processing | Progress: 31%\n",
            "Status: processing | Progress: 41%\n",
            "Status: processing | Progress: 62%\n",
            "Status: processing | Progress: 72%\n",
            "Status: processing | Progress: 72%\n",
            "Status: processing | Progress: 82%\n",
            "Status: processing | Progress: 93%\n",
            "Status: completed | Progress: 100%\n",
            "✅ Job completed!\n",
            "\n",
            "3️⃣ Retrieving results...\n",
            "Status: completed | Progress: 100%\n",
            "✅ Job complete. Results found directly in API response.\n",
            "📄 Retrieved 249294 characters of markdown.\n",
            "💾 Saved to: cmgmnkyer0004f4ee511uo9oc_output.md\n",
            "\n",
            "📊 Processing stats:\n",
            "  • Pages: 97\n",
            "  • Time: 260.0s\n",
            "  • Credits: 291.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwedk6C1CMly"
      },
      "source": [
        "## 📋 Quick Reference\n",
        "\n",
        "### API Endpoints:\n",
        "```\n",
        "POST /v1/ade/parse/jobs         → Submit document\n",
        "GET  /v1/ade/parse/jobs/{job_id} → Check status\n",
        "GET  /v1/ade/parse/jobs    → List all jobs\n",
        "```\n",
        "\n",
        "### Python Workflow:\n",
        "```python\n",
        "# 1. Submit\n",
        "job_id = submit_document('doc.pdf', API_KEY)\n",
        "\n",
        "# 2. Wait\n",
        "result = wait_for_completion(job_id, API_KEY)\n",
        "\n",
        "# 3. Retrieve\n",
        "markdown = get_results(job_id, API_KEY)\n",
        "```\n",
        "\n",
        "### 💡 Tips:\n",
        "- Poll every 10-30 seconds for large documents\n",
        "- Save job_id immediately after submission\n",
        "- Check file size before submission (<1GB)\n",
        "- Use exponential backoff for retries\n",
        "\n",
        "### 🔗 Resources:\n",
        "- Docs: https://docs.landing.ai/ade\n",
        "- API Key: https://va.landing.ai/settings/api-key\n",
        "\n",
        "---\n",
        "\n",
        "**That's it! You're ready to process large documents with the Async Parse API.** 🎉"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}