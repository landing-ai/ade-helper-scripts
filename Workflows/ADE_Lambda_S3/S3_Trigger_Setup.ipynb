{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 Trigger Setup - Automatic Document Processing with ADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# LandingAI Applied AI Content Notebook Template\n",
    "# ---\n",
    "# Title: S3 Trigger Setup - Automatic Document Processing with ADE\n",
    "# Author: Ava Xia\n",
    "# Description: Streamlined notebook for testing and using the deployed Lambda function\n",
    "# Target Audience: [Developers, Partners, Customers]\n",
    "# Content Type: [Tutorial, How-To]\n",
    "# Publish Date: 2025-09-23\n",
    "# ADE Version: v0.1.5\n",
    "# Change Log:\n",
    "#    - v1.0: Initial draft\n",
    "#    - v1.1: Modularized with utility functions\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook configures automatic processing when documents are uploaded to S3.\n",
    "\n",
    "**⚠️ IMPORTANT:** Make sure your AWS session is in the same region as your Lambda function and S3 bucket (typically `us-east-2`). \n",
    "\n",
    "If using AWS SSO, login with:\n",
    "```bash\n",
    "aws sso login --profile your-profile-name\n",
    "export AWS_DEFAULT_REGION=us-east-2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How S3 Triggers Work\n",
    "\n",
    "```\n",
    "┌──────────┐      ┌──────────┐      ┌──────────┐      ┌──────────┐\n",
    "│  Upload  │ ---> │    S3    │ ---> │  Trigger │ ---> │  Lambda  │\n",
    "│   PDF    │      │  Bucket  │      │  Event   │      │ Process  │\n",
    "└──────────┘      └──────────┘      └──────────┘      └──────────┘\n",
    "                                           │                 │\n",
    "                                           ↓                 ↓\n",
    "                                    Automatic         Save Results\n",
    "```\n",
    "\n",
    "When you upload a PDF to the configured S3 folder:\n",
    "1. S3 generates an event\n",
    "2. Event triggers Lambda function\n",
    "3. Lambda processes the document\n",
    "4. Results saved to `ade-results/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔧 Initializing AWS environment...\n",
      "============================================================\n",
      "✅ AWS Environment configured\n",
      "   Profile: workload-dev-2\n",
      "   Region: us-east-2\n",
      "   Account: 9700XXXX1993\n",
      "\n",
      "✅ Environment ready!\n",
      "   Lambda: ade-lambda-s3\n",
      "   Bucket: cf-mle-testing\n",
      "   Region: us-east-2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Import consolidated utilities and configuration\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display, JSON\n",
    "\n",
    "# Import consolidated modules\n",
    "from config import get_settings\n",
    "from utils import (\n",
    "    setup_aws_environment,\n",
    "    list_s3_files,\n",
    "    check_lambda_environment,\n",
    "    setup_s3_trigger,\n",
    "    get_lambda_invocation_stats,\n",
    "    get_error_logs\n",
    ")\n",
    "\n",
    "# Initialize environment using config.py and .env\n",
    "print(\"=\"*60)\n",
    "print(\"🔧 Initializing AWS environment...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load configuration (automatically reads from .env)\n",
    "config, clients, AWS_ACCOUNT_ID, aws_session = setup_aws_environment()\n",
    "\n",
    "# Check if credentials are valid\n",
    "if AWS_ACCOUNT_ID in ['EXPIRED', 'ERROR']:\n",
    "    print(\"\\n⚠️  Please refresh your AWS credentials:\")\n",
    "    print(\"   aws sso login --profile your-profile-name\")\n",
    "    print(\"   export AWS_DEFAULT_REGION=us-east-2\")\n",
    "else:\n",
    "    # Extract configuration values\n",
    "    BUCKET_NAME = config['bucket_name']\n",
    "    FUNCTION_NAME = config['function_name']\n",
    "    ECR_REPO = config['ecr_repo']\n",
    "    AWS_REGION = config['aws_region']\n",
    "    \n",
    "    print(\"\\n✅ Environment ready!\")\n",
    "    print(f\"   Lambda: {FUNCTION_NAME}\")\n",
    "    print(f\"   Bucket: {BUCKET_NAME}\")\n",
    "    print(f\"   Region: {AWS_REGION}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure S3 Trigger\n",
    "\n",
    "Set up automatic processing for a specific folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Setting up S3 trigger for: invoices/\n",
      "\n",
      "ℹ️  S3 permission already exists\n",
      "✅ S3 trigger configured!\n",
      "   📤 Upload PDFs to: s3://cf-mle-testing/invoices/\n",
      "   ⚡ They will auto-process with Lambda\n",
      "\n",
      "✅ S3 trigger is active!\n",
      "   Any PDF uploaded to s3://cf-mle-testing/invoices/\n",
      "   will be automatically processed\n"
     ]
    }
   ],
   "source": [
    "# Configure S3 trigger for invoices folder\n",
    "trigger_folder = \"invoices/\"\n",
    "\n",
    "print(f\"🎯 Setting up S3 trigger for: {trigger_folder}\")\n",
    "print()\n",
    "\n",
    "success = setup_s3_trigger(\n",
    "    clients['s3'],\n",
    "    clients['lambda'],\n",
    "    BUCKET_NAME,\n",
    "    FUNCTION_NAME,\n",
    "    folder=trigger_folder\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\n✅ S3 trigger is active!\")\n",
    "    print(f\"   Any PDF uploaded to s3://{BUCKET_NAME}/{trigger_folder}\")\n",
    "    print(\"   will be automatically processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the Trigger\n",
    "\n",
    "Upload a test file to verify the trigger works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading test file...\n",
      "   From: input_folder/invoice_1.pdf\n",
      "   To: s3://cf-mle-testing/invoices/test_trigger_20250925_153109.pdf\n",
      "\n",
      "✅ File uploaded successfully!\n",
      "⏳ Waiting for Lambda to process...\n",
      "📂 Files in s3://cf-mle-testing/ade-results/\n",
      "Found 5 files\n",
      "\n",
      "✅ Processing complete!\n",
      "   Recent results:\n",
      "   • ade-results/batch_extracted_20250924_195832.json\n",
      "   • ade-results/batch_extracted_20250925_013951.json\n",
      "   • ade-results/batch_extracted_20250925_014051.json\n"
     ]
    }
   ],
   "source": [
    "# Test file upload\n",
    "import time  # Add missing import\n",
    "\n",
    "test_file_path = \"input_folder/invoice_1.pdf\"  # Local file\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "s3_key = f\"{trigger_folder}test_trigger_{timestamp}.pdf\"\n",
    "\n",
    "print(\"📤 Uploading test file...\")\n",
    "print(f\"   From: {test_file_path}\")\n",
    "print(f\"   To: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Upload the file\n",
    "    clients['s3'].upload_file(test_file_path, BUCKET_NAME, s3_key)\n",
    "    print(\"✅ File uploaded successfully!\")\n",
    "    print(\"⏳ Waiting for Lambda to process...\")\n",
    "    \n",
    "    # Wait for processing\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Check for results\n",
    "    results = list_s3_files(\n",
    "        clients['s3'],\n",
    "        BUCKET_NAME,\n",
    "        \"ade-results/\",\n",
    "        max_files=5\n",
    "    )\n",
    "    \n",
    "    if results:\n",
    "        print(\"\\n✅ Processing complete!\")\n",
    "        print(\"   Recent results:\")\n",
    "        for r in results[:3]:\n",
    "            print(f\"   • {r['File']}\")\n",
    "    else:\n",
    "        print(\"⚠️ Still processing or no results yet\")\n",
    "        print(\"   Check CloudWatch logs for details\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Test file not found: {test_file_path}\")\n",
    "    print(\"   Please update the path to a valid PDF file\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Upload failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitor Lambda Invocations\n",
    "\n",
    "Check Lambda statistics to see trigger activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Lambda Invocation Statistics (last 1 hours)\n",
      "==================================================\n",
      "   Total Invocations: 1\n",
      "\n",
      "📊 Recent Activity (last hour):\n",
      "   Invocations: 1\n",
      "   Success rate: 100%\n"
     ]
    }
   ],
   "source": [
    "# Get Lambda invocation stats\n",
    "stats = get_lambda_invocation_stats(\n",
    "    clients['logs'],\n",
    "    FUNCTION_NAME,\n",
    "    hours_back=1  # Last hour\n",
    ")\n",
    "\n",
    "if stats.get('total_invocations', 0) > 0:\n",
    "    print(f\"\\n📊 Recent Activity (last hour):\")\n",
    "    print(f\"   Invocations: {stats['total_invocations']}\")\n",
    "    print(f\"   Success rate: {stats.get('success_rate', 0):.0f}%\")\n",
    "else:\n",
    "    print(\"\\nℹ️ No recent invocations\")\n",
    "    print(\"   Upload a file to trigger processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check for Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error Logs (last 1 hours)\n",
      "==================================================\n",
      "✅ No errors found\n",
      "✅ No errors in the last hour\n"
     ]
    }
   ],
   "source": [
    "# Check for recent errors\n",
    "errors = get_error_logs(\n",
    "    clients['logs'],\n",
    "    FUNCTION_NAME,\n",
    "    hours_back=1\n",
    ")\n",
    "\n",
    "if not errors:\n",
    "    print(\"✅ No errors in the last hour\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {len(errors)} errors\")\n",
    "    print(\"   Check CloudWatch logs for details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Remove S3 Trigger\n",
    "\n",
    "If needed, remove the S3 trigger configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ To remove triggers, uncomment the code above\n"
     ]
    }
   ],
   "source": [
    "# Remove S3 trigger (uncomment to run)\n",
    "\"\"\"\n",
    "print(\"🗑️ Removing S3 trigger...\")\n",
    "\n",
    "try:\n",
    "    # Clear bucket notifications\n",
    "    clients['s3'].put_bucket_notification_configuration(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        NotificationConfiguration={}\n",
    "    )\n",
    "    print(\"✅ S3 trigger removed\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error removing trigger: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ℹ️ To remove triggers, uncomment the code above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices\n",
    "\n",
    "### Folder Organization\n",
    "```\n",
    "s3://bucket/\n",
    "├── invoices/        # Auto-process with invoice schema\n",
    "├── receipts/        # Auto-process with receipt schema\n",
    "├── documents/       # Auto-process with parsing mode\n",
    "├── ade-results/     # Processing results\n",
    "└── archive/         # Processed files (optional)\n",
    "```\n",
    "\n",
    "### Tips\n",
    "1. **Use specific folders** for different document types\n",
    "2. **Monitor CloudWatch** for processing logs\n",
    "3. **Set up SNS** for processing notifications\n",
    "4. **Archive processed files** to avoid reprocessing\n",
    "5. **Use versioning** on S3 bucket for safety"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
